{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=contents></a>\n",
    "\n",
    "# Model building\n",
    "\n",
    "In this notebook I'm loading up the BERTopic model that I had trained on GoogleColab, merging the results of the model with the rest of the data and exploring the results. \n",
    "\n",
    "\n",
    "[1. Loading up the training data and model](#one1)\n",
    "\n",
    "[2. Exploring and reducing the topics](#two)\n",
    "\n",
    "[3. Linkages in Topics](#three)\n",
    "\n",
    "[4. Our main topics over time](#four)\n",
    "\n",
    "[4.1 Sentiment over time](#five)\n",
    "\n",
    "[4.2 Model 2](#two)\n",
    "\n",
    "[4.2 Model 3](#three)\n",
    "\n",
    "[4.2 Model 4](#four)\n",
    "\n",
    "[4.2 Model 5](#five)\n",
    "\n",
    "[7. Conclusions and model comparison table](#conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=one1></a>\n",
    "\n",
    "## 1. Load up training data and model\n",
    "\n",
    "[LINK to table of contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functions.topic_modelling_BertTopic import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import joblib\n",
    "from umap import UMAP\n",
    "\n",
    "from scipy.stats import hmean\n",
    "import plotly\n",
    "import torch\n",
    "import pickle, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading up docs\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12037 entries, 1580168615357140992 to 1565189065158311937\n",
      "Data columns (total 19 columns):\n",
      " #   Column                       Non-Null Count  Dtype              \n",
      "---  ------                       --------------  -----              \n",
      " 0   Unnamed: 0.2                 12037 non-null  int64              \n",
      " 1   Unnamed: 0.1                 12037 non-null  int64              \n",
      " 2   datetime                     12037 non-null  datetime64[ns, UTC]\n",
      " 3   display_name                 12037 non-null  object             \n",
      " 4   tweet_text                   12037 non-null  object             \n",
      " 5   User_id                      12037 non-null  float64            \n",
      " 6   #likes                       12037 non-null  float64            \n",
      " 7   #retweets                    12037 non-null  float64            \n",
      " 8   #responses                   12037 non-null  float64            \n",
      " 9   language                     12037 non-null  object             \n",
      " 10  extracted_twitter_handles    12037 non-null  object             \n",
      " 11  extracted_URLs               12037 non-null  object             \n",
      " 12  extracted_hashtags           12037 non-null  object             \n",
      " 13  Before_or_after_controversy  12037 non-null  object             \n",
      " 14  num_tokens                   12037 non-null  int64              \n",
      " 15  len_text                     12037 non-null  int64              \n",
      " 16  By_or_at_Musk                12037 non-null  object             \n",
      " 17  Unnamed: 0                   12037 non-null  int64              \n",
      " 18  clean_tweet_text             12037 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(4), int64(5), object(9)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "print('Loading up docs')\n",
    "filepath = 'data/clean/features/text_for_topics_post_Aug22.csv'\n",
    "docs = pd.read_csv(filepath, index_col='tweet_id')\n",
    "\n",
    "#meta data getting loaded too\n",
    "filepath = 'data/clean/dashboard_data.csv'\n",
    "df = pd.read_csv(filepath, index_col='tweet_id').drop(columns=['Unnamed: 0', 'clean_tweet_text'],)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.join(docs, how='right')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bytes' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ipreoteasa/Desktop/Io/blog/IoanaFio/content/project/twitter_sentiment_tracking/2. Topic_modelling_with_BERT.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ipreoteasa/Desktop/Io/blog/IoanaFio/content/project/twitter_sentiment_tracking/2.%20Topic_modelling_with_BERT.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m topic_model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmodels/topic/BertTopic/topic_model_final.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr+\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/btopic/lib/python3.10/site-packages/joblib/numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    582\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    583\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    584\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 587\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[1;32m    588\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/btopic/lib/python3.10/site-packages/joblib/numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    507\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[1;32m    508\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[1;32m    512\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/btopic/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/btopic/lib/python3.10/pickle.py:1590\u001b[0m, in \u001b[0;36m_Unpickler.load_reduce\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m args \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39mpop()\n\u001b[1;32m   1589\u001b[0m func \u001b[39m=\u001b[39m stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1590\u001b[0m stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/btopic/lib/python3.10/site-packages/numba/core/serialize.py:97\u001b[0m, in \u001b[0;36m_unpickle__CustomPickled\u001b[0;34m(serialized)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unpickle__CustomPickled\u001b[39m(serialized):\n\u001b[1;32m     93\u001b[0m     \u001b[39m\"\"\"standard unpickling for `_CustomPickled`.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[39m    Uses `NumbaPickler` to load.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     ctor, states \u001b[39m=\u001b[39m loads(serialized)\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m _CustomPickled(ctor, states)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bytes' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "topic_model = joblib.load('models/topic/BertTopic/topic_model_final.pkl', 'r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=two></a>\n",
    "\n",
    "## 2. Exploring and reducing topics\n",
    "\n",
    "[LINK to table of contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three></a>\n",
    "\n",
    "## 3. Linkages in topics\n",
    "\n",
    "[LINK to table of contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four></a>\n",
    "\n",
    "## 4. Our main topics over time\n",
    "\n",
    "[LINK to table of contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five></a>\n",
    "\n",
    "## 5. Sentiment over time\n",
    "\n",
    "[LINK to table of contents](#contents)\n",
    "\n",
    "On GoogleColab, I had also used a pre-trained [sentiment_classifier](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) from BERT in order to infer the sentiment of the tweets collected. Let's explore how sentiment varies across the other dimensions of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11914 entries, 1580168615357140992 to 1565189065158311937\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype              \n",
      "---  ------                --------------  -----              \n",
      " 0   clean_tweet_text      11914 non-null  object             \n",
      " 1   Pred_sentiment_out    11914 non-null  object             \n",
      " 2   Pred_sentiment_score  11914 non-null  float64            \n",
      " 3   datetime              11914 non-null  datetime64[ns, UTC]\n",
      " 4   By_or_at_Musk         11914 non-null  object             \n",
      " 5   language              11914 non-null  object             \n",
      " 6   #likes                11914 non-null  float64            \n",
      " 7   #retweets             11914 non-null  float64            \n",
      " 8   #responses            11914 non-null  float64            \n",
      " 9   display_name          11914 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(4), object(5)\n",
      "memory usage: 1023.9+ KB\n"
     ]
    }
   ],
   "source": [
    "sent = pd.read_csv('data/preds/text_and_sentiment_preds.csv', index_col='tweet_id')\n",
    "sent = sent.join(df[['datetime', 'By_or_at_Musk', 'language', '#likes', '#retweets', '#responses','display_name']])\n",
    "# dropping non-english tweets\n",
    "sent = sent.loc[sent.language=='en']\n",
    "sent.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>By_or_at_Musk</th>\n",
       "      <th>Pred_sentiment_out</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-01 00:00:00+00:00</td>\n",
       "      <td>By @elonmusk</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-01 00:00:00+00:00</td>\n",
       "      <td>Mentions @elonmusk</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-01 00:00:00+00:00</td>\n",
       "      <td>Mentions @elonmusk</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-02 00:00:00+00:00</td>\n",
       "      <td>Mentions @elonmusk</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-02 00:00:00+00:00</td>\n",
       "      <td>Mentions @elonmusk</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime       By_or_at_Musk Pred_sentiment_out   0\n",
       "0 2022-09-01 00:00:00+00:00        By @elonmusk           POSITIVE   3\n",
       "1 2022-09-01 00:00:00+00:00  Mentions @elonmusk           NEGATIVE  44\n",
       "2 2022-09-01 00:00:00+00:00  Mentions @elonmusk           POSITIVE  23\n",
       "3 2022-09-02 00:00:00+00:00  Mentions @elonmusk           NEGATIVE  25\n",
       "4 2022-09-02 00:00:00+00:00  Mentions @elonmusk           POSITIVE  10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_agg = sent[['datetime', 'By_or_at_Musk', 'Pred_sentiment_out']].groupby([pd.Grouper(key='datetime', freq='D'), \n",
    "                        'By_or_at_Musk', 'Pred_sentiment_out']).size().reset_index()\n",
    "                        \n",
    "                        # agg({'Pred_sentiment_out':'count', \n",
    "                        #                         # 'Pred_sentiment_score':lambda x : hmean(x),\n",
    "                        #                         '#likes': 'mean', \n",
    "                        #                         '#retweets':'mean',\n",
    "                        #                         '#responses':'mean'\n",
    "                        #                         }).reset_index()\n",
    "# sent_agg_melt = sent_agg.melt(id_vars=['datetime', 'By_or_at_Musk'])\n",
    "sent_agg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentiment over time, agg'd by day and before and after \n",
    "\n",
    "plotly.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=conc ><a/> \n",
    "\n",
    "## 7. Conclusions and model comparison table\n",
    "    \n",
    "[LINK to table of contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('btopic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbebbce43ef9648808cf557fe1380068f4309693ead3e309523e0ff7efa5d8c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
